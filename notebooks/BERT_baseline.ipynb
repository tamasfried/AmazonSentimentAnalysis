{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21449baf",
   "metadata": {},
   "source": [
    "# Transformer-based baseline (BERT)\n",
    "\n",
    "A transformer-based sentiment classification model is evaluated using a pre-trained BERT architecture. This notebook establishes a deep learning baseline to compare against classical machine learning approaches based on TF-IDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a51c7",
   "metadata": {},
   "source": [
    "## Load dataset and create train–test split\n",
    "\n",
    "The same cleaned and balanced review dataset used for the classical machine learning baselines is loaded. The train–test split is reproduced to ensure a fair comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c687085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"../data/balanced_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7731d89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fd1bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text and labels\n",
    "X = df[\"Text\"]\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "# Create a reproducible train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057dcb4",
   "metadata": {},
   "source": [
    "## Tokenisation using a pre-trained BERT tokenizer\n",
    "Text data is converted into tokens that the BERT model can understand. This step prepares the text for input into the BERT model by converting words into numerical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bcbf0b",
   "metadata": {},
   "source": [
    "### Configure local Hugging Face cache directory\n",
    "\n",
    "The Hugging Face Transformers library downloads pre-trained model files to a local cache directory. On some systems, the default cache location may not be writable, which can cause permission errors during model download. A project-local cache directory is configured to ensure reliable and reproducible access to pre-trained model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00de80a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tommorton/Library/CloudStorage/OneDrive-Personal/Masters/Comp Sci/Modules/Masters Project/AmazonSentinmentAnalysis/notebooks/.hf_cache'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set a local Hugging Face cache directory inside the project\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "\n",
    "# Ensure the cache directory exists\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f74f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    cache_dir=os.path.join(os.getcwd(), \".hf_cache\")\n",
    ")\n",
    "\n",
    "# Tokenise the training and test sets\n",
    "train_encodings = tokenizer(\n",
    "    X_train.tolist(), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=128\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    X_test.tolist(),\n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89545da",
   "metadata": {},
   "source": [
    "## Encode labels\n",
    "Sentiment labels are converted into a numerical format sothat they can be used by transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c460f9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': np.int64(0), 'neutral': np.int64(1), 'positive': np.int64(2)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the sentiment labels as intergers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "# Display mapping of sentiment labels to integers\n",
    "dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84c2b6",
   "metadata": {},
   "source": [
    "## Create PyTorch dataset objects\n",
    "The tokenised text and encoded labels need to be wrapped in a PyTorch Dataset object for use with the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a81482b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000, 24000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a dataset wrapped for tokenised inputs\n",
    "class ReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[index])\n",
    "        return item\n",
    "\n",
    "# Create dataset objects for training and test sets\n",
    "train_dataset = ReviewsDataset(train_encodings, y_train_enc)\n",
    "test_dataset = ReviewsDataset(test_encodings, y_test_enc)\n",
    "\n",
    "# Check split size\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfe9cf",
   "metadata": {},
   "source": [
    "## Load pre-trained BERT model for sequence classification\n",
    "Load the pre-trained BERT model is loaded with a classification head suited for the multiclass sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "585ad82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 778.02it/s, Materializing param=bert.pooler.dense.weight]                               \n",
      "BertForSequenceClassification LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label_encoder.classes_), # Not hard coding allows for change in number of classes\n",
    "    cache_dir=os.path.join(os.getcwd(), \".hf_cache\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8222220",
   "metadata": {},
   "source": [
    "## WIP - Define training parameters\n",
    "Training arguments are defined to control how the transformer model is fine tuned. These settings include the number of epochs, batch sizes, learning rate, and evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801803de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
