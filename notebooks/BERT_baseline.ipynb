{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21449baf",
   "metadata": {},
   "source": [
    "# Transformer-based baseline (BERT)\n",
    "\n",
    "A transformer-based sentiment classification model is evaluated using a pre-trained BERT architecture. This notebook establishes a deep learning baseline to compare against classical machine learning approaches based on TF-IDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a51c7",
   "metadata": {},
   "source": [
    "## Load dataset and create train–test split\n",
    "\n",
    "The same cleaned and balanced review dataset used for the classical machine learning baselines is loaded. The train–test split is reproduced to ensure a fair comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c687085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"../data/balanced_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7731d89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd1bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text and labels\n",
    "X = df[\"Text\"]\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "# Create a reproducible train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057dcb4",
   "metadata": {},
   "source": [
    "## Tokenisation using a pre-trained BERT tokenizer\n",
    "Text data is converted into tokens that the BERT model can understand. This step prepares the text for input into the BERT model by converting words into numerical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bcbf0b",
   "metadata": {},
   "source": [
    "### Configure local Hugging Face cache directory\n",
    "\n",
    "The Hugging Face Transformers library downloads pre-trained model files to a local cache directory. On some systems, the default cache location may not be writable, which can cause permission errors during model download. A project-local cache directory is configured to ensure reliable and reproducible access to pre-trained model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00de80a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tommorton/Library/CloudStorage/OneDrive-Personal/Masters/Comp Sci/Modules/Masters Project/AmazonSentinmentAnalysis/notebooks/.hf_cache'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set a local Hugging Face cache directory inside the project\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(os.getcwd(), \".hf_cache\")\n",
    "\n",
    "# Ensure the cache directory exists\n",
    "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f74f140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    cache_dir=os.path.join(os.getcwd(), \".hf_cache\")\n",
    ")\n",
    "\n",
    "# Tokenise the training and test text\n",
    "train_encodings = tokenizer(\n",
    "    X_train.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    X_test.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89545da",
   "metadata": {},
   "source": [
    "## Encode labels\n",
    "Sentiment labels are converted into a numerical format sothat they can be used by transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c460f9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': np.int64(0), 'neutral': np.int64(1), 'positive': np.int64(2)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the sentiment labels as intergers\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "# Display mapping of sentiment labels to integers\n",
    "dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84c2b6",
   "metadata": {},
   "source": [
    "## Create PyTorch dataset objects\n",
    "The tokenised text and encoded labels need to be wrapped in a PyTorch Dataset object for use with the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81482b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000, 24000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a dataset wrapped for tokenised inputs\n",
    "class ReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[index])\n",
    "        return item\n",
    "\n",
    "# Create dataset objects for training and test sets\n",
    "train_dataset = ReviewsDataset(train_encodings, y_train_enc)\n",
    "test_dataset = ReviewsDataset(test_encodings, y_test_enc)\n",
    "\n",
    "# Check split size\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbfe9cf",
   "metadata": {},
   "source": [
    "## Load pre-trained BERT model for sequence classification\n",
    "Load the pre-trained BERT model is loaded with a classification head suited for the multiclass sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "585ad82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1490.42it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Load a pre-trained DistilBERT model with a classification head\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(label_encoder.classes_), # Not hard coding allows for easy extension to multi-class\n",
    "    cache_dir=os.path.join(os.getcwd(), \".hf_cache\")\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8222220",
   "metadata": {},
   "source": [
    "## WIP - Define training parameters\n",
    "Training arguments are defined to control how the transformer model is fine tuned. These settings include the number of epochs, batch sizes, learning rate, and evaluation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "801803de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=True,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=False,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "enable_jit_checkpoint=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=IntervalStrategy.EPOCH,\n",
       "eval_use_gather_object=False,\n",
       "fp16=False,\n",
       "fp16_full_eval=False,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_revision=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_num_input_tokens_seen=no,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "liger_kernel_config=None,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=None,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=IntervalStrategy.EPOCH,\n",
       "lr_scheduler_kwargs=None,\n",
       "lr_scheduler_type=SchedulerType.LINEAR,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "neftune_noise_alpha=None,\n",
       "num_train_epochs=2,\n",
       "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=../results/bert_baseline,\n",
       "parallelism_config=None,\n",
       "per_device_eval_batch_size=16,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "project=huggingface,\n",
       "push_to_hub=False,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=None,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_steps=500,\n",
       "save_strategy=SaveStrategy.NO,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "trackio_space_id=trackio,\n",
       "use_cache=False,\n",
       "use_cpu=False,\n",
       "use_liger_kernel=False,\n",
       "warmup_ratio=None,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define training arguments for fine-tuning and evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results/bert_baseline\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=False # Silence error when running on Apple Silicon Macs\n",
    ")\n",
    "\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f89049",
   "metadata": {},
   "source": [
    "## Create a trainer and define the evaluation metrics\n",
    "A trainer object can be created to manage the fine tuning and evaluation. A metric function is also defined to calculate accuracy and macro averaged F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8d512c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x3d15e0e10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define evaluation metrics for model performance\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": f1\n",
    "    }\n",
    "\n",
    "# Create Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29c09bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 03:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0984047651290894,\n",
       " 'eval_model_preparation_time': 0.0042,\n",
       " 'eval_accuracy': 0.3385416666666667,\n",
       " 'eval_macro_f1': 0.2006771513132997,\n",
       " 'eval_runtime': 193.6398,\n",
       " 'eval_samples_per_second': 123.941,\n",
       " 'eval_steps_per_second': 7.746}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the pre-trained BERT model before fine-tuning\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1c11422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='24000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   28/24000 00:05 < 1:29:20, 4.47 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fine-tune the BERT model on the training dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Started training BERT model. Switching to DistilBERT due to resource constraints.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/amazon-sentiment/lib/python3.11/site-packages/transformers/trainer.py:2174\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2172\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/amazon-sentiment/lib/python3.11/site-packages/transformers/trainer.py:2541\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2535\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2536\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2539\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2540\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2541\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2542\u001b[39m ):\n\u001b[32m   2543\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2544\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune the BERT model on the training dataset\n",
    "trainer.train()\n",
    "# Started training BERT model. Switching to DistilBERT due to resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ea93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
